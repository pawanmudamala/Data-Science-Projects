# -*- coding: utf-8 -*-
"""gold_project_model_(1)_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nIJqn6GDhUkN0YZWqEObnkf5_JBVpOT-

## **import** **required** **libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df= pd.read_csv("gold data (2018-2023).csv")

df.dtypes

df.head(25)

# Preprocess Dates
df['Date'] = df['Date'].str.replace('/', '-')  # Replace '/' with '-'

# Convert to Datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

# Display the DataFrame
print(df)

df.head(20)

df.info()

df.describe()

df.isnull().sum()

df.head()

df.shape

"""# **Check for Normalisation in Data**"""

sns.distplot(df['High'])
plt.axvline(x=np.mean(df['High']), c='red', ls='--', label='mean')
plt.axvline(x=np.percentile(df['High'],25),c='green', ls='--', label = '25th percentile:Q1')
plt.axvline(x=np.percentile(df['High'],75),c='orange', ls='--',label = '75th percentile:Q3' )
plt.legend()

sns.distplot(df['Close'])
plt.axvline(x=np.mean(df['Close']), c='red', ls='--', label='mean')
plt.axvline(x=np.percentile(df['Close'],25),c='green', ls='--', label = '25th percentile:Q1')
plt.axvline(x=np.percentile(df['Close'],75),c='orange', ls='--',label = '75th percentile:Q3' )
plt.legend()

sns.distplot(df['Open'])
plt.axvline(x=np.mean(df['Open']), c='red', ls='--', label='mean')
plt.axvline(x=np.percentile(df['Open'],25),c='green', ls='--', label = '25th percentile:Q1')
plt.axvline(x=np.percentile(df['Open'],75),c='orange', ls='--',label = '75th percentile:Q3' )
plt.legend()

sns.distplot(df['Adj Close'])
plt.axvline(x=np.mean(df['Adj Close']), c='red', ls='--', label='mean')
plt.axvline(x=np.percentile(df['Adj Close'],25),c='green', ls='--', label = '25th percentile:Q1')
plt.axvline(x=np.percentile(df['Adj Close'],75),c='orange', ls='--',label = '75th percentile:Q3' )
plt.legend()

sns.distplot(df['Volume'])
plt.axvline(x=np.mean(df['Volume']), c='red', ls='--', label='mean')
plt.axvline(x=np.percentile(df['Volume'],25),c='green', ls='--', label = '25th percentile:Q1')
plt.axvline(x=np.percentile(df['Volume'],75),c='orange', ls='--',label = '75th percentile:Q3' )
plt.legend()

sns.distplot(df['Low'])
plt.axvline(x=np.mean(df['Low']), c='red', ls='--', label='mean')
plt.axvline(x=np.percentile(df['Low'],25),c='green', ls='--', label = '25th percentile:Q1')
plt.axvline(x=np.percentile(df['Low'],75),c='orange', ls='--',label = '75th percentile:Q3' )
plt.legend()

"""# Distibution of Histogram"""

plt.hist(df["High"])
plt.show()

plt.hist(df["Low"])
plt.show()

"""# Boxplot"""

df.boxplot(column=["High"])

df.boxplot(column=["Open"])

"""# Check Trend in Data"""

plt.figure(figsize=(15,6))
series1=sns.lineplot(x='High',y='Low',data=df)
plt.show(series1)

!pip install matplotlib-venn

!apt-get -qq install -y libfluidsynth1

"""# Line plot"""

# create a line plot
from pandas import read_csv
from matplotlib import pyplot

import pandas as pd
import matplotlib.pyplot as plt

# Assuming 'data' is your DataFrame with 'date' as the index and 'price' as the column to plot
df.plot(kind='line',x='Date', y='Close', figsize=(10, 6), title='Line Plot of Price Over Time')

# Customize the plot (optional)
plt.xlabel('Date')
plt.ylabel('Close')

# Show the plot
plt.show()

"""# Histogram plot"""

# create a histogram plot
from pandas import read_csv
from matplotlib import pyplot

series = read_csv('gold data (2018-2023).csv', header=0, index_col=0,parse_dates=True)
series.hist()
pyplot.show()

"""# Density plot"""

# create a density plot
from pandas import read_csv
from matplotlib import pyplot
series = read_csv('gold data (2018-2023).csv', header=0, index_col=0,parse_dates=True)
series.plot(kind='kde')
pyplot.show()

"""# Trend and seasonal using seaborn lib"""

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame
sns.catplot(x='Date', y='Close', data=df, kind='point')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import seaborn as sns

print(sns.__version__)

pip install seaborn --upgrade

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'df' is your DataFrame
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month

# Use catplot
sns.catplot(data=df, x="Month", y="Close", hue="Year", kind="point")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# #### Let visualizing the sum of all sales each year. We can do that using group of “Close” and “Date” and group by “Year”."""

df=df
df['year'] = df['Date'].dt.year
df['month'] = df['Date'].dt.month
df['day'] = df['Date'].dt.day
df['weekday'] = df['Date'].dt.weekday
df['weekday'] = np.where(df.weekday == 0, 7, df.weekday)
df_year = df[['Close','Year']].groupby(by='Year').sum().reset_index()

df_year

"""# Catplot"""

sns.catplot(x='Year',y='Close',data=df_year,kind='bar',aspect=2)

"""# Monthly & Yeary Gold Price (TREND & SEASONALITY)"""

# Draw Plot
fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)
sns.boxplot(x='Year', y='Close', data=df, ax=axes[0])
sns.boxplot(x='Month', y='Close', data=df.loc[~df.Year.isin([2018, 2021]), :])

# Set Title
axes[0].set_title('Year-wise Box Plot\n(The Trend)', fontsize=18);
axes[1].set_title('Month-wise Box Plot\n(The Seasonality)', fontsize=18)
plt.show()

"""# Weekly gold price"""

plot = sns.boxplot(x='weekday', y='Close', data=df)
plot.set(title='Weekly Gold Close')

"""# Time series Decompostion plot"""

from statsmodels.tsa.seasonal import seasonal_decompose
import matplotlib
data=df.sort_values(by='Date', ascending=True)
data=df.set_index('Date')
matplotlib.rc('figure', figsize=(7, 7))
decompose = seasonal_decompose(df.Close, period=10, model='additive')
decompose.plot()
plt.show()

"""# Check Stationarity of a Time Series"""

from statsmodels.tsa.stattools import adfuller
def test_stationarity(timeseries):

    #Determing rolling statistics
    rolmean = timeseries.rolling(12).mean()
    rolstd = timeseries.rolling(12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show(block=False)

test_stationarity(data)

import seaborn as sns
import matplotlib.pyplot as plt

def correlation_heatmap(data):
    # Compute the correlation matrix
    corr = data.corr()

    # Generate a mask for the upper triangle
    mask = np.triu(np.ones_like(corr, dtype=bool))

    # Set up the matplotlib figure
    plt.figure(figsize=(10, 8))

    # Draw the heatmap with the mask and correct aspect ratio
    sns.heatmap(corr, mask=mask, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title('Correlation Heatmap')
    plt.show()

# Assuming 'df' is your DataFrame containing the dataset
correlation_heatmap(df)

df.head()

df2 = df[['Date', 'Close']]

df2.head()

df2 = df2.rename(columns={'Close': 'Price'})

from sklearn.model_selection import train_test_split

from sklearn.model_selection import train_test_split
# separate out a validation dataset
split_point = len(df2)-152
dataset, validation = df2[0:split_point], df2[split_point:]
print('Dataset %d, Validation %d' % (len(dataset), len(validation)))
dataset.to_csv('dataset.csv', header=False)
validation.to_csv('validation.csv', header=False)

import pandas as pd

# Read the dataset
df3 = pd.read_csv('dataset.csv', header=None, parse_dates=[0], index_col=0)

# Convert the second column (index 1) to float32
df3.iloc[:, 1] = df3.iloc[:, 1].astype('float32')

# Extract values from DataFrame
X = df3.iloc[:, 1].values

# Split data into train and test sets
train_size = int(len(X) * 0.50)
train, test = X[0:train_size], X[train_size:]

from sklearn.metrics import mean_squared_error
from math import sqrt
# walk-forward validation
history = [x for x in train]
predictions = list()
for i in range(len(test)):
    yhat = history[-1]
    predictions.append(yhat)
# observation
    obs = test[i]
    history.append(obs)
    print('>Predicted=%.3f, Expected=%.3f' % (yhat, obs))
# report performance
rmse = sqrt(mean_squared_error(test, predictions))
print('RMSE: %.3f' % rmse)

decompose_ts_add = seasonal_decompose(df2.Price,period=12)
decompose_ts_add.plot()
plt.show()

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plt.rcParams.update({'figure.figsize':(17,12), 'figure.dpi':120})

# Original Series
fig, axes = plt.subplots(3, 2)
axes[0, 0].plot(df2.Price); axes[0, 0].set_title('Original Series')
plot_acf(df2.Price, ax=axes[0, 1])

# 1st Differencing
axes[1, 0].plot(df2.Price.diff()); axes[1, 0].set_title('1st Order Differencing')
plot_acf(df2.Price.diff().dropna(), ax=axes[1, 1])

# 2nd Differencing
axes[2, 0].plot(df2.Price.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')
plot_acf(df2.Price.diff().diff().dropna(), ax=axes[2, 1])

plt.show()

!pip install pmdarima

#check number of difference required for a stationary series

from pmdarima.arima.utils import ndiffs
y = df2.Price
## Adf Test
print('ADF :', ndiffs(y, test='adf') )
# KPSS test
print('KPSS :' ,ndiffs(y, test='kpss'))
# PP test:
print('PP :' ,ndiffs(y, test='pp'))

# PACF plot of 1st differenced series
plt.rcParams.update({'figure.figsize':(15,5), 'figure.dpi':120})

fig, axes = plt.subplots(1, 2)
axes[0].plot(df2.Price.diff()); axes[0].set_title('1st Differencing')
axes[1].set(ylim=(0,5))
plot_pacf(df2.Price.diff().dropna(), ax=axes[1])

plt.show()

fig, axes = plt.subplots(1, 2)
axes[0].plot(df2.Price.diff()); axes[0].set_title('1st Differencing')
axes[1].set(ylim=(0,1.2))
plot_acf(df2.Price.diff().dropna(), ax=axes[1])

plt.show()

from statsmodels.tsa.arima_model import ARIMA

# separate out a validation dataset
split_point = len(data)-152
dataset, validation = df2[0:split_point], df2[split_point:]
print('Dataset %d, Validation %d' % (len(dataset), len(validation)))
dataset.to_csv('dataset.csv', header=False)
validation.to_csv('validation.csv', header=False)

df2.info()

# load data
train = read_csv('dataset.csv', header=0, index_col=0, parse_dates=True)
# prepare data
X = train.iloc[:, 1].values

X

!pip install --upgrade statsmodels

from statsmodels.tsa.arima.model import ARIMA

# Fit ARIMA model
model = ARIMA(X, order=(0, 1, 0))
model_fit = model.fit()

# Forecast
forecast = model_fit.forecast(steps=152)

# Plot residual errors
residuals = pd.DataFrame(model_fit.resid)
fig, ax = plt.subplots(1,2)
residuals.plot(title="Residuals", ax=ax[0])
residuals.plot(kind='kde', title='Density', ax=ax[1])
plt.show()

import matplotlib.pyplot as plt

# Plot actual values
plt.plot(X, label='Actual')

# Plot fitted values
plt.plot(model_fit.fittedvalues, color='red', label='Fitted')

plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Actual vs. Fitted Values')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error
from math import sqrt
val=pd.read_csv('validation.csv',header=None)

# Assuming val[1] contains the actual values
actual_values = val[2]

# Calculate the mean squared error
mse = mean_squared_error(actual_values, forecast)

# Calculate the root mean squared error (RMSE)
rmse = sqrt(mse)
rmse

from statsmodels.tsa.stattools import acf

# Create Training and Test
train = df2.Price[:558].astype('float32')
test = df2.Price[558:].astype('float32')

import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Build Model
model = ARIMA(train, order=(0, 1, 0))
fitted = model.fit()

# Forecast
fc = fitted.forecast(steps=674, alpha=0.05)  # 95% conf

# Make as pandas series
fc_series = pd.Series(fc, index=test.index)

# Plot
plt.figure(figsize=(12,5), dpi=100)
plt.plot(train, label='training')
plt.plot(test, label='actual')
plt.plot(fc_series, label='forecast')
plt.title('Forecast vs Actuals')
plt.legend(loc='upper left', fontsize=8)
plt.show()

import numpy as np

# Accuracy metrics function
def forecast_accuracy(forecast, actual):
    mape = np.mean(np.abs(forecast - actual) / np.abs(actual)) * 100  # Mean Absolute Percentage Error (MAPE)
    me = np.mean(forecast - actual)             # Mean Error (ME)
    mae = np.mean(np.abs(forecast - actual))    # Mean Absolute Error (MAE)
    mpe = np.mean((forecast - actual) / actual) * 100   # Mean Percentage Error (MPE)
    rmse = np.mean((forecast - actual) ** 2) ** 0.5  # Root Mean Squared Error (RMSE)
    corr = np.corrcoef(forecast, actual)[0, 1]   # Correlation between forecast and actual values
    minmax = 1 - np.mean(np.minimum(forecast / actual, actual / forecast))   # Min-Max Accuracy
    return {'MAPE': mape, 'ME': me, 'MAE': mae, 'MPE': mpe, 'RMSE': rmse, 'Corr': corr, 'Min-Max Accuracy': minmax}

# Ensure the shapes of forecast and actual arrays are the same
if len(fc) != len(test.values):
    raise ValueError("The forecast and actual arrays must have the same length.")

# Using the function
accuracy_metrics = forecast_accuracy(fc, test.values)
print(accuracy_metrics)

import pmdarima as pm

model = pm.auto_arima(df2.Price, start_p=1, start_q=1,
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=1,
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

print(model.summary())

model.plot_diagnostics(figsize=(7,5))
plt.show()

# Forecast
n_periods = 20
fc, confint = model.predict(n_periods=n_periods, return_conf_int=True)
index_of_fc = np.arange(len(df2.Price), len(df2.Price)+n_periods)

# make series for plotting purpose
fc_series = pd.Series(fc, index=index_of_fc)
lower_series = pd.Series(confint[:, 0], index=index_of_fc)
upper_series = pd.Series(confint[:, 1], index=index_of_fc)

# Plot
plt.plot(df2.Price,)
plt.plot(fc_series, color='darkgreen')
plt.fill_between(lower_series.index,
                 lower_series,
                 upper_series,
                 color='k', alpha=.15)

plt.title("Final Forecast Usage")
plt.show()

# Convert index to numeric data type
fc_series.index = pd.to_numeric(fc_series.index)
lower_series.index = pd.to_numeric(lower_series.index)
upper_series.index = pd.to_numeric(upper_series.index)

# Plot
plt.figure(figsize=(12,5), dpi=100)
plt.plot(train, label='training')
plt.plot(test, label='actual')
plt.plot(fc_series, label='forecast')
plt.fill_between(lower_series.index, lower_series, upper_series,
                 color='k', alpha=.15)
plt.title('Forecast vs Actuals')
plt.legend(loc='upper left', fontsize=8)
plt.show()

import matplotlib.pyplot as plt

# Plot
fig, axes = plt.subplots(2, 1, figsize=(17,7), dpi=100)

# Usual Differencing
axes[0].plot(data[:], label='Original Series')
axes[0].plot(data[:].diff(2), label='Usual Differencing')
axes[0].set_title('Usual Differencing')
axes[0].legend(loc='upper left', fontsize=10)

# Seasonal Differencing
axes[1].plot(data[:], label='Original Series')
axes[1].plot(data[:].diff(12), label='Seasonal Differencing', color='green')
axes[1].set_title('Seasonal Differencing')
axes[1].legend(loc='upper left', fontsize=10)

plt.show()

# !pip3 install pyramid-arima
import pmdarima as pm
# Assuming df2 is your DataFrame with timestamps in the index
# Reset the index to convert the timestamps to a column
df2.reset_index(drop=True, inplace=True)

# Convert the timestamp column to numerical values
#df2['timestamp'] = df2['timestamp'].astype(np.int64) // 10**9

# Drop any other non-numeric columns
df2 = df2.select_dtypes(include=['number'])

# Now you can pass df2 to auto_arima
smodel = pm.auto_arima(df2, start_p=1, start_q=1,
                       max_p=3, max_q=3, m=12,
                       start_P=0, seasonal=True,
                       d=None, D=1, trace=True,
                       error_action='ignore',
                       suppress_warnings=True,
                       stepwise=True)

smodel.summary()

import pmdarima as pm

# Seasonal - fit stepwise auto-ARIMA
smodel = pm.auto_arima(df2, start_p=1, start_q=1,
                         max_p=3, max_q=3, m=12,
                         start_P=0, seasonal=True,
                         d=None, D=1, trace=True,
                         error_action='ignore',
                         suppress_warnings=True,
                         stepwise=True)

smodel.summary()

import pmdarima as pm
# Forecast
n_periods = 24
fitted, confint = model.predict(n_periods=n_periods, return_conf_int=True)
# Get the last timestamp of the original series
last_timestamp = df2.index[-1]

# Increment the last timestamp to represent future dates
index_of_fc = pd.date_range(last_timestamp, periods=n_periods, freq='MS')


# make series for plotting purpose
fitted_series = pd.Series(fitted, index=index_of_fc)
lower_series = pd.Series(confint[:, 0], index=index_of_fc)
upper_series = pd.Series(confint[:, 1], index=index_of_fc)

# Plot
plt.plot(df2)
plt.plot(fitted_series, color='darkgreen')
plt.fill_between(lower_series.index,
                 lower_series,
                 upper_series,
                 color='k', alpha=.15)

plt.title("SARIMA - Final Forecast")
plt.show()

import pickle
import pmdarima as pm

# Train the ARIMA model
model = pm.auto_arima(df2.Price, start_p=1, start_q=1,
                      max_p=3, max_q=3, # maximum p and q
                      m=1,              # frequency of series
                      d=1,
                      seasonal=False,   # No Seasonality
                      start_P=0,
                      D=0,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=True)

# Forecast
n_periods = 20
forecast, conf_int = model.predict(n_periods=n_periods, return_conf_int=True)

# Save the trained model to a file
with open('arima_model.pkl', 'wb') as f:
    pickle.dump((model, forecast, conf_int), f)

